{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in Python\n",
    "\n",
    "Dans cet exercice, nous allons utiliser différentes méthodes de traitement automatique du langage naturel dites \"distributionelle\". Ces méthodes s'appuient sur l'analyse de grands corpus (ensemble de documents) dans l'objectif d'extraire de la connaissance du texte selon la distribution de chaque mots par rapport aux autres.\n",
    "\n",
    "La tâche choisie (Semantic Textual Similarity) est une tâche de regression linéaire, nous allons créé un modèle capable de prédire la similarité sémantique de 2 phrases. Chaque paire du jeu de données a été annotée par des humains dans l'interval `[0, 5]` selon la similarité sémantique des 2 phrases composant la paire.\n",
    "\n",
    "*Références :*\n",
    "\n",
    " * [La page wiki de la tâche STS](http://ixa2.si.ehu.es/stswiki/index.php/Main_Page)\n",
    " * [L'article expliquant en détail la tâche et présentant les systèmes vainqueurs](https://www.aclweb.org/anthology/S16-1081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # package permettant de trouver des chemins selon des patterns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous commençons par trouver les chemins vers les fichiers :\n",
    "trainFilesPath = glob.glob(\"./data/201[0-5]*.tsv\")\n",
    "testFilesPath = glob.glob(\"./data/2016*.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit une fonction qui lit les fichiers ligne par ligne et renvoie les données :\n",
    "def parseFiles(filesPath):\n",
    "    data = []\n",
    "    for path in filesPath:\n",
    "        for line in open(path):\n",
    "            if len(line) > 1:\n",
    "                try:\n",
    "                    line = line.split(\"\\t\")\n",
    "                    data.append((float(line[0]), line[1].strip(), line[2].strip()))\n",
    "                except: pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les données :\n",
    "trainData = parseFiles(trainFilesPath)\n",
    "random.shuffle(trainData)\n",
    "testData = parseFiles(trainFilesPath)\n",
    "random.shuffle(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puis nous créons le train set et les test set dans 4 listes :\n",
    "trainPairs = []\n",
    "trainLabels = []\n",
    "for current in trainData:\n",
    "    trainLabels.append(current[0])\n",
    "    trainPairs.append((current[1], current[2]))\n",
    "testPairs = []\n",
    "testLabels = []\n",
    "for current in testData:\n",
    "    testLabels.append(current[0])\n",
    "    testPairs.append((current[1], current[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** : Commencez par afficher 4 paires de phrases très similaires (> 4.5) et 4 très différentes (< 0.5) uniquement dans le train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.75\n",
      "Unrestricted freedom to use something.\n",
      "unrestricted freedom to use.\n",
      "\n",
      "0.4\n",
      "3 dead, 4 missing in central China construction accident\n",
      "One dead, 8 missing in Vietnam boat accident\n",
      "\n",
      "4.6\n",
      "Jesse Jackson Jr. and wife to plead guilty to fraud\n",
      "Jesse Jackson Jr., Wife to Plead Guilty to Fraud\n",
      "\n",
      "5.0\n",
      "terminal one is connected to the negative battery terminal\n",
      "terminal 1 is connected to the negative battery terminal\n",
      "\n",
      "0.2\n",
      "Two men standing in grass staring at a car.\n",
      "A woman in a pink top posing with beer.\n",
      "\n",
      "5.0\n",
      "Two dogs play in the grass.\n",
      "Two dogs are playing in the grass.\n",
      "\n",
      "5.0\n",
      "Run at a moderately swift pace, as for exercise.\n",
      "run at a moderately swift pace.\n",
      "\n",
      "0.4\n",
      "A society fraying at the edges\n",
      "Old TV alignments crack at the edges\n",
      "\n",
      "0.2\n",
      "i agree with there goes trouble.\n",
      "i am done with this thread.\n",
      "\n",
      "0.2\n",
      "Obama nominates new transportation secretary\n",
      "Obama To Meet Embattled Veterans Secretary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correction :\n",
    "simPrintCount = 0\n",
    "disPrintCount = 0\n",
    "for i in range(len(trainPairs)):\n",
    "    pair = trainPairs[i]\n",
    "    label = trainLabels[i]\n",
    "    if simPrintCount <= 4 and label > 4.5:\n",
    "        print(str(label) + \"\\n\" + pair[0] + \"\\n\" + pair[1] + \"\\n\")\n",
    "        simPrintCount += 1\n",
    "    elif disPrintCount <= 4 and label < 0.5:\n",
    "        print(str(label) + \"\\n\" + pair[0] + \"\\n\" + pair[1] + \"\\n\")\n",
    "        disPrintCount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** : Convertissez en *minuscule* et *tokenizez* chaque phrase du jeu de donnée train et test. La conversion en minuscule permet de réduire la taille du vocabulaire (le nombre de mots différents dans le corpus). La tokenization transforme un texte en liste de mots. Indice : l'utilisation de la librairie [`nltk`](https://www.nltk.org/index.html) permet d'effectuer une tokenization plus complète en, notamment, séparant `\"don't\"` et 2 mots `\"do\"` et `\"n't\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction :\n",
    "import nltk\n",
    "for dataset in [trainPairs, testPairs]:\n",
    "    for i in range(len(dataset)):\n",
    "        left = nltk.word_tokenize(dataset[i][0].lower())\n",
    "        right = nltk.word_tokenize(dataset[i][1].lower())\n",
    "        dataset[i] = (left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['as', 'part', 'of', 'a', 'restructuring', 'peregrine', 'sold', 'its', 'remedy', 'help', 'desk', 'software', 'unit', 'last', 'year', 'to', 'bmc', 'software', 'inc', '.'], ['peregrine', 'sold', 'its', 'remedy', 'business', 'unit', 'to', 'bmc', 'software', 'in', 'november', 'for', '$', '355', 'million', '.'])\n",
      "(['nm', 'county', 'prepares', 'for', 'same-sex', 'marriages', 'hearing'], ['some', 'county', 'officials', 'pave', 'the', 'way', 'for', 'same-sex', 'marriage'])\n"
     ]
    }
   ],
   "source": [
    "# On affiche la première paire du train et la première paire du test :\n",
    "print(trainPairs[0])\n",
    "print(testPairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3 (facultatif) :** Faites une [lemmatization](https://fr.wikipedia.org/wiki/Lemmatisation) ou un stemming de chaque mot du jeu de données. Le stemming et la lemmatization ont pour objectif de rassembler les mots proches en une forme commune. Le principe de stemming est d'enlever le début ou la fin d'un mot en utilisant une liste de préfixes et suffixes. Par exemple : `\"studies\"` deviendra `\"studi\"` (suffixe `\"es\"`) et `\"studying\"` deviendra `\"study\"` (suffixe `\"ing\"`). La lemmatization prend en compte la morphologie et les informations grammaticales du mot afin de le convertir en une forme commune. Par exemple `\"studies\"` deviendra `\"study\"` et `\"went\"` deviendra `\"go\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction :\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for dataset in [trainPairs, testPairs]:\n",
    "    for pairIndex in range(len(dataset)):\n",
    "        pair = dataset[pairIndex]\n",
    "        for sentenceIndex in [0, 1]:\n",
    "            sentence = pair[sentenceIndex]\n",
    "            for wordIndex in range(len(sentence)):\n",
    "                sentence[wordIndex] = lemmatizer.lemmatize(sentence[wordIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['unrestricted', 'freedom', 'to', 'use', 'something', '.'], ['unrestricted', 'freedom', 'to', 'use', '.'])\n",
      "(['nm', 'county', 'prepares', 'for', 'same-sex', 'marriage', 'hearing'], ['some', 'county', 'official', 'pave', 'the', 'way', 'for', 'same-sex', 'marriage'])\n"
     ]
    }
   ],
   "source": [
    "# On affiche la première paire du train et la première paire du test :\n",
    "print(trainPairs[1])\n",
    "print(testPairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de *baselines*\n",
    "\n",
    "L'objectif des *baselines* est d'avoir une base comparative (en terme de score) pour notre modèle final à des méthodes naïves, classiques ou aléatoires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4 (facultatif) :** Le premier système que nous allons implémenter va prédire la similarité des phrases en renvoyant, pour une paire données de phrase, un nombre aléatoire entre 0 et 5. Implementez la fonction `randomBaseline(tokens1, tokens2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction :\n",
    "def randomBaseline(tokens1, tokens2):\n",
    "    return round(random.uniform(0, 5), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.92 -> (['a', 'part', 'of', 'a', 'restructuring', 'peregrine', 'sold', 'it', 'remedy', 'help', 'desk', 'software', 'unit', 'last', 'year', 'to', 'bmc', 'software', 'inc', '.'], ['peregrine', 'sold', 'it', 'remedy', 'business', 'unit', 'to', 'bmc', 'software', 'in', 'november', 'for', '$', '355', 'million', '.'])\n",
      "0.85 -> (['unrestricted', 'freedom', 'to', 'use', 'something', '.'], ['unrestricted', 'freedom', 'to', 'use', '.'])\n",
      "0.88 -> (['this', 'gross', 'error', 'is', 'leading', 'russia', 'to', 'political', 'ruin', '.'], ['and', 'this', 'mistake', 'mistake', 'is', 'in', 'the', 'process', 'of', 'it', 'political', '.'])\n"
     ]
    }
   ],
   "source": [
    "# On test sur quelques paires :\n",
    "for i in range(3):\n",
    "    pair = trainPairs[i]\n",
    "    print(str(randomBaseline(*pair)) + \" -> \" + str(pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5 :** La seconde baseline consiste à renvoyer un ratio (ramenez entre 0 et 5) du nombre de mots en commun sur la longueur de la paire la plus courte. Implémentez la fonction `commonWordsBaseline(tokens1, tokens2)`. Testez sur la première paire du train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction :\n",
    "def commonWordsBaseline(tokens1, tokens2):\n",
    "    commonCount = 0\n",
    "    for word in tokens1:\n",
    "        if word in tokens2:\n",
    "            commonCount += 1\n",
    "    return commonCount / min(len(tokens1), len(tokens2)) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.125 -> (['a', 'part', 'of', 'a', 'restructuring', 'peregrine', 'sold', 'it', 'remedy', 'help', 'desk', 'software', 'unit', 'last', 'year', 'to', 'bmc', 'software', 'inc', '.'], ['peregrine', 'sold', 'it', 'remedy', 'business', 'unit', 'to', 'bmc', 'software', 'in', 'november', 'for', '$', '355', 'million', '.'])\n",
      "5.0 -> (['unrestricted', 'freedom', 'to', 'use', 'something', '.'], ['unrestricted', 'freedom', 'to', 'use', '.'])\n",
      "2.0 -> (['this', 'gross', 'error', 'is', 'leading', 'russia', 'to', 'political', 'ruin', '.'], ['and', 'this', 'mistake', 'mistake', 'is', 'in', 'the', 'process', 'of', 'it', 'political', '.'])\n"
     ]
    }
   ],
   "source": [
    "# On test sur quelques paires :\n",
    "for i in range(3):\n",
    "    pair = trainPairs[i]\n",
    "    print(str(commonWordsBaseline(*pair)) + \" -> \" + str(pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des systèmes\n",
    "\n",
    "Pour l'évaluation d'un système, nous allons comparer le vecteur *output* (produit par le système sur les paires du jeu de données test) avec le *gold standard* (les annotations humaines). Pour cela nous allons utiliser la corrélation de Pearson entre les 2 vecteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6 :** Générez les listes `randomOutput` et `commonWordsOutput` qui convertie `testPairs` en scores de similarité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction :\n",
    "randomOutput = []\n",
    "commonWordsOutput = []\n",
    "for pair in testPairs:\n",
    "    randomOutput.append(randomBaseline(*pair))\n",
    "    commonWordsOutput.append(commonWordsBaseline(*pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction de corrélation de Pearson :\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomBaseline score: 0.003891555230542661\n"
     ]
    }
   ],
   "source": [
    "# On évalue le randomBaseline :\n",
    "print(\"randomBaseline score: \" + str(pearsonr(randomOutput, testLabels)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonWordsBaseline score: 0.5827417638140329\n"
     ]
    }
   ],
   "source": [
    "# On évalue le randomBaseline :\n",
    "print(\"commonWordsBaseline score: \" + str(pearsonr(commonWordsOutput, testLabels)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de vecteurs de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La machine n'a, contrairement aux humains, aucune connaissance du langage, elle n'est pas capable de trouver de proximité sémantique entre des mots comme \"professeur\" et \"enseignant\". Une première méthode est de créer manuellement un dictionnaire de synonymes. Mais la tâche est complexe et les méthode distributionelles permettent d'exploiter de grands corpus pour automatiquement inferer des similarités entre mots.\n",
    "\n",
    "La sémantique distributionelle se fonde sur l'hypothèse de Harris énnoncé en 1954 : les mots ayant statistiquemet le même contexte seront plus probablement similaire sémantiquement. Concretement, cela signifie que les mots ayant les même voisins dans les phrases sont souvent sémantiquement proches. Par exemple les mots \"voiture\" et \"moto\" sont souvent voisins du verbe \"rouler\", on estimera alors que \"moto\" et \"voiture\" sont sémantiquement proches.\n",
    "\n",
    "Afin de rassembler les mots d'un corpus, il est necessaire de pouvoirLes mots \n",
    "\n",
    "Pourquoi la notion de similarité est importante ?\n",
    "\n",
    "*Références :*\n",
    "\n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous récoltons toutes les phrases du corpus :\n",
    "corpus = []\n",
    "for dataset in [trainPairs, testPairs]:\n",
    "    for pair in dataset:\n",
    "        corpus.append(pair[0])\n",
    "        corpus.append(pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice X :** A l'aide de la [documentation de `Word2Vec`](https://radimrehurek.com/gensim/models/word2vec.html), entrainez un modèle `Word2Vec` sur la variable `corpus` (liste de phrases). Une fois entrainé, ce modèle est capable de générer un vecteur pour chaque mot du vocabulaire du corpus. La classe `Word2Vec` fournie également des méthodes utiles pour analyser des similarités de mots etc. Le constructeur de la classe prend en premier paramètre une liste de phrases (donc une liste de liste de mots). Vous utiliserez les paramètres `size=50, window=3, min_count=1, iter=10`, `window` correspond à une fenetre de contexte de 3 mots à gauche et à droite pour chaque mot cible, `size` permet de spécifier la dimension ds vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction :\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(corpus, size=100, window=5, min_count=1, iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation qualitative du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice X :** Grâce à la documentation, trouvez comment afficher les mots les plus similaires à ces mots : `[\"car\", \"play\", \"house\", \"apparently\", \"coin\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar of car:\n",
      "['jet', 'bus', 'wheel', 'truck', 'jacket', 'bicycle', 'horse', 'shirt', 'plane', 'camouflaged']\n",
      "Most similar of play:\n",
      "['dance', 'pose', 'compete', 'live', 'get', 'choose', 'find', 'look', 'manage', 'sink']\n",
      "Most similar of house:\n",
      "['temple', 'sanctuary', 'sepulchre', 'chamber', 'gate', 'room', 'palace', 'hall', 'garden', 'door']\n",
      "Most similar of apparently:\n",
      "['obvious', 'evidently', 'totally', 'curiously', 'acknowledged', 'denying', 'encouraging', 'imminent', 'unmoved', 'prompted']\n",
      "Most similar of coin:\n",
      "['bullion', 'jewel', 'gold', 'silver', 'rgld', 'pegasus', 'ecu', 'lvnvf', 'hemlo', 'platinum']\n"
     ]
    }
   ],
   "source": [
    "# Correction :\n",
    "for word in [\"car\", \"play\", \"house\", \"apparently\", \"coin\"]:\n",
    "    print(\"Most similar of \" + word + \":\\n\" + str([x[0] for x in model.wv.similar_by_word(word)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration du modèle\n",
    "\n",
    "Nous pouvons remarquer que le modèle représente mal les mots `\"apparently\"` et `\"coin\"`. Le problème de ce modèle est qu'il a été entrainé sur un jeu de données contenant uniquement ~50000 phrases. Or, pour mieux représenter chaque mots en vecteur, `Word2Vec` a besoin d'observer un grand nombre d'occurences pour chaque mots afin d'avoir le plus de contexte possible. Nous allons donc ajouter un corpus externe à notre ensemble de phrases pour améliorer les représentation vectoielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/hayj/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# On charge toutes les phrases du corpus gutenberg et reuters :\n",
    "from nltk.corpus import gutenberg, reuters\n",
    "nltk.download('reuters')\n",
    "nltkSentences = []\n",
    "for nltkCorpus in [gutenberg, reuters]:\n",
    "    for sentence in nltkCorpus.sents():\n",
    "        s = []\n",
    "        for w in sentence:\n",
    "            w = w.lower()\n",
    "            try:\n",
    "                w = lemmatizer.lemmatize(w)\n",
    "            except: pass\n",
    "            s.append(w)\n",
    "        nltkSentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 48368\n",
      "corpus new length: 201636\n"
     ]
    }
   ],
   "source": [
    "# On archive l'ancien modèle et ajoute les phrases de gutenberg au corpus :\n",
    "oldModel = model\n",
    "print(\"corpus length: \" + str(len(corpus)))\n",
    "corpus += nltkSentences\n",
    "print(\"corpus new length: \" + str(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice X :** Ré-executez l'entrainement de `Word2Vec` sur `corpus` afin de créer un nouveau modèle. La représentation de `\"apparently\"` et `\"coin\"` vous parait elle meilleure ? Vous pouvez utiliser `oldModel` pour comparer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un système basé sur notre modèle `Word2Vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice X :** A l'aide de la [documentation de `Word2Vec`](https://radimrehurek.com/gensim/models/word2vec.html), implémentez la fonction `vecSystem(tokens1, tokens2, model)` qui renvoie la similarité (généralament similarité cosinus) de 2 phases. Génerez le vecteur `word2vecSystemOutput` sur `testPairs`. Facultatif : comparez le score de `model` et `oldModel` (entrainé sur uniquement les paires de phrases STS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction :\n",
    "def vecSystem(tokens1, tokens2, model):\n",
    "    return model.wv.n_similarity(tokens1, tokens2) * 5.0\n",
    "word2vecSystemOutput = []\n",
    "for pair in testPairs:\n",
    "    word2vecSystemOutput.append(word2vecSystem(*pair, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vecSystem score: 0.34612107936552633\n"
     ]
    }
   ],
   "source": [
    "# On évalue vecSystem+model :\n",
    "print(\"vecSystem+model score: \" + str(pearsonr(word2vecSystemOutput, testLabels)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration du modèle avec `Doc2Vec`\n",
    "\n",
    "`Word2Vec` est très utilisé pour de la similarité de mots et pour le [transfert learning](https://en.wikipedia.org/wiki/Transfer_learning) mais n'est pas adapté à la similarité de phrases. En effet, une phrase est la somme ou la moyenne de ses mots. Afin de représenter un document ou une phrase, l'outil le plus adapté est [`Doc2Vec`](https://radimrehurek.com/gensim/models/doc2vec.html). Une fois entrainé, `Doc2Vec` peut générer des vecteurs pour n'importe quel ensemble de mots (présent dans le corpus ou non).\n",
    "\n",
    "*Références :*\n",
    "\n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous entrainons un modèle `Doc2Vec` sur notre corpus :\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "d2vCorpus = []\n",
    "i = 0\n",
    "for sentence in corpus:\n",
    "    d2vCorpus.append(TaggedDocument(sentence, [i]))\n",
    "    i += 1\n",
    "d2vModel = Doc2Vec(d2vCorpus, vector_size=100, window=5, min_count=1, epochs=10, negative=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice X :** En utilisant la fonction deja implémentée `vecSystem` et le nouveau model `d2vModel`, génerez le vecteur `doc2vecSystemOutput` sur `testPairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vecSystemOutput = []\n",
    "for pair in testPairs:\n",
    "    doc2vecSystemOutput.append(vecSystem(*pair, d2vModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecSystem+d2vModel score: 0.3461210848199269\n"
     ]
    }
   ],
   "source": [
    "# On évalue vecSystem avec d2vModel :\n",
    "print(\"vecSystem+d2vModel score: \" + str(pearsonr(doc2vecSystemOutput, testLabels)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vModel = Doc2Vec.load(\"/home/hayj/tmp/d2v/model1/model1.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "clf = Ridge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'$' in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        vocabulary.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for pair in trainPairs:\n",
    "    tokens1 = pair[0]\n",
    "    tokens1 = [x for x in tokens1 if x in d2vModel.wv.vocab]\n",
    "    tokens2 = pair[1]\n",
    "    tokens2 = [x for x in tokens2 if x in d2vModel.wv.vocab]\n",
    "    X.append([vecSystem(tokens1, tokens2, d2vModel), commonWordsBaseline(*pair)]) # , commonWordsBaseline(*pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(np.array(X), np.array(trainLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures = []\n",
    "for pair in testPairs:\n",
    "    tokens1 = pair[0]\n",
    "    tokens1 = [x for x in tokens1 if x in d2vModel.wv.vocab]\n",
    "    tokens2 = pair[1]\n",
    "    tokens2 = [x for x in tokens2 if x in d2vModel.wv.vocab]\n",
    "    features = [vecSystem(tokens1, tokens2, d2vModel), commonWordsBaseline(*pair)] # , commonWordsBaseline(*pair)\n",
    "    testFeatures.append(features)\n",
    "predictions = []\n",
    "predictions = clf.predict(np.array(testFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.60751254, 1.98812251, 1.30188731, ..., 3.67023716, 3.30160898,\n",
       "       4.03210398])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecSystem+d2vModel score: 0.6108947188380894\n"
     ]
    }
   ],
   "source": [
    "# On évalue vecSystem avec d2vModel :\n",
    "print(\"vecSystem+d2vModel score: \" + str(pearsonr(predictions, testLabels)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "with open(\"/home/hayj/Downloads/glove.6B/glove.6B.100d.txt\") as f:\n",
    "     for line in f:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        values = tokens[1:]\n",
    "        assert len(word) > 0\n",
    "        assert len(values) > 3\n",
    "        vector = np.asarray(values, dtype='float32')\n",
    "        vectors[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokensToEmbedding(tokens, wordVectors=None, operation='sum', removeDuplicates=True, doLower=False):\n",
    "    \"\"\"\n",
    "        This function take tokens (or a list of tokens)\n",
    "        And a map word->vector\n",
    "        It return a sentence embedding according to the operation given (sum, mean).\n",
    "    \"\"\"\n",
    "    if wordVectors is None:\n",
    "        wordVectors = getWordVectorsSingleton().load()\n",
    "    if isinstance(tokens[0], list):\n",
    "        nbDocs = len(tokens)\n",
    "        mtx = None\n",
    "        tokens = copy.deepcopy(tokens)\n",
    "        for i in range(len(tokens)):\n",
    "            currentArray = tokensToEmbedding(tokens[i], wordVectors=wordVectors, operation=operation,\n",
    "                                          removeDuplicates=removeDuplicates, doLower=doLower)\n",
    "            if mtx is None:\n",
    "                mtx = currentArray\n",
    "            else:\n",
    "                mtx = np.vstack((mtx, currentArray))\n",
    "        return mtx\n",
    "    else:\n",
    "        if removeDuplicates:\n",
    "            tokens = set(tokens)\n",
    "        vectors = []\n",
    "        for current in tokens:\n",
    "            if doLower:\n",
    "                current = current.lower()\n",
    "            if current in wordVectors:\n",
    "                vectors.append(wordVectors[\"current\"])\n",
    "        if operation == 'sum':\n",
    "            return np.sum(np.array(vectors), axis=0)\n",
    "        elif operation == 'mean':\n",
    "            return np.mean(np.array(vectors), axis=0)\n",
    "        print(vectors.shape)\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.0761805 ,   2.1492002 ,  11.865239  ,  -2.7417598 ,\n",
       "        -2.8045793 ,  -9.705238  ,  -5.5060205 ,   2.78082   ,\n",
       "       -12.238922  ,   3.2299201 ,  -1.264806  ,  -3.1912196 ,\n",
       "         7.512659  ,  -4.07304   ,   7.475579  , -11.485261  ,\n",
       "        -2.0471396 ,  -3.8401206 ,   4.3178406 ,  -6.295861  ,\n",
       "        -3.2688005 ,  -6.1333213 ,   5.6597414 ,   7.802459  ,\n",
       "        -2.8753197 , -18.026999  ,   6.0303607 , -13.55148   ,\n",
       "         2.16648   ,   3.2095797 ,   0.31086   ,  13.881779  ,\n",
       "        -4.305601  ,  -2.37132   , -11.346662  ,   9.053818  ,\n",
       "         4.928399  ,  -3.38436   ,  -3.6844199 ,  -5.8928404 ,\n",
       "        -6.940621  ,  -6.5125813 ,  11.636999  ,  -0.75366   ,\n",
       "        10.211579  ,   6.7483788 ,   2.3452203 , -12.47022   ,\n",
       "       -10.292402  ,  -9.496979  ,  10.6713    ,  -5.6662188 ,\n",
       "         2.1279597 ,  12.565978  ,   6.3867617 , -47.259     ,\n",
       "         5.19714   ,  -5.801039  ,  27.806398  ,   5.3109    ,\n",
       "         1.9360802 ,  -2.50614   ,  -3.2633998 ,  -1.0864258 ,\n",
       "        20.6154    ,   1.9238396 ,  -1.135332  ,  -1.076922  ,\n",
       "         6.8769    ,   0.09766259,  10.93662   ,   2.96118   ,\n",
       "        -2.7795596 ,  -3.7342796 ,   2.8857603 ,  -9.16848   ,\n",
       "         0.9149938 ,   2.7862198 , -16.40286   ,  -4.94262   ,\n",
       "         7.371541  ,   2.41434   ,  -3.0614395 ,  -2.6731799 ,\n",
       "       -20.3382    ,   1.6822621 ,  -4.92606   ,  -5.4500403 ,\n",
       "         8.67762   ,  -8.623802  ,  -1.4909041 ,  12.202922  ,\n",
       "        -8.570339  ,  -2.6170204 ,  -7.1667    ,  -7.454879  ,\n",
       "        -1.7620016 ,  -7.265338  ,  18.120605  ,  -1.225818  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensToEmbedding(trainPairs[0][0], vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
